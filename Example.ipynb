{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic and instance segmentation of images\n",
    "This shows an example of a combined approach to doing _semantic_ and _instance_ segmentation of images. For this example of microscope slides containing lice, this means segmenting the image such that each segment represents a unique object (an _instance_) and that that object is from a particular (_semantic_) class. In the case of these slides, the classes are: background, specimens, labels, barcodes, and type labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "from segmentation.datasets import Slides, ImageFolder\n",
    "from segmentation.instances import DiscriminativeLoss, mean_shift, visualise_embeddings, visualise_instances\n",
    "from segmentation.network import SemanticInstanceSegmentation\n",
    "from segmentation.training import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "The model is a neural network with two heads: one for the semantic class embeddings, and one for the instance embedding. A discriminative loss function is used that encourages embeddings from the same instance to be closer to each other than to an embedding from any other instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemanticInstanceSegmentation().cuda()\n",
    "instance_clustering = DiscriminativeLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomCrop((256, 768)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "target_transform = transforms.Compose([transform, transforms.Lambda(lambda x: (x * 255).long())])\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# WARNING: Don't use multiple workers for loading! Doesn't work with setting random seed\n",
    "train_data_labelled = Slides(download=True, train=True, root='data', transform=transform, target_transform=target_transform)\n",
    "train_loader_labelled = torch.utils.data.DataLoader(train_data_labelled, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "train_data_unlabelled = ImageFolder(root='data/slides', transform=transform)\n",
    "train_loader_unlabelled = torch.utils.data.DataLoader(train_data_unlabelled, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "\n",
    "test_data_labelled = Slides(download=True, train=False, root='data', transform=transform, target_transform=target_transform)\n",
    "test_loader_labelled = torch.utils.data.DataLoader(test_data_labelled, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "test_data_unlabelled = ImageFolder(root='data/slides', transform=transform)\n",
    "test_loader_unlabelled = torch.utils.data.DataLoader(test_data_unlabelled, batch_size=batch_size, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(model, instance_clustering, train_loader_labelled, train_loader_unlabelled, test_loader_labelled, test_loader_unlabelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/epoch_100'))\n",
    "model.eval()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_labelled, batch_size=1, shuffle=True)\n",
    "\n",
    "image, labels, instances = next(iter(train_loader))\n",
    "\n",
    "image = Variable(image)\n",
    "instances = Variable(instances + 1)\n",
    "_, logits, instance_embeddings = model.forward_clean(image.cuda())\n",
    "\n",
    "current_logits = logits[0]\n",
    "current_labels = labels[0, 0].cuda()\n",
    "current_instances = instances[0].cuda()\n",
    "\n",
    "predicted_class = current_logits.data.max(0)[1]\n",
    "predicted_instances = [None] * 5\n",
    "for class_index in range(5):\n",
    "    mask = predicted_class.view(-1) == class_index\n",
    "    if mask.max() > 0:\n",
    "        label_embedding = instance_embeddings[0].view(1, instance_embeddings.shape[1], -1)[..., mask]\n",
    "        label_embedding = label_embedding.data.cpu().numpy()[0]\n",
    "\n",
    "        predicted_instances[class_index] = mean_shift(label_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise training results\n",
    "Note that for _semantic_ segmentation the colours correspond to semantic classes, whereas for _instance_ segmentation the colours represent unique instances that can be in an arbitrary order - hence the ID number (colour) won't be the same as in the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'Paired'\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "for ax in axes.flatten(): ax.axis('off')\n",
    "\n",
    "axes[0, 0].set_title('Original image')\n",
    "axes[0, 0].imshow(image[0].data.numpy().transpose(1, 2, 0))\n",
    "axes[1, 0].set_title('Ground truth classes')\n",
    "axes[1, 0].imshow(current_labels.cpu().numpy().squeeze())\n",
    "axes[2, 0].set_title('Ground truth instances')\n",
    "axes[2, 0].imshow(current_instances.cpu().numpy().squeeze())\n",
    "axes[1, 1].set_title('Predicted classes')\n",
    "axes[1, 1].imshow(predicted_class.cpu().numpy().squeeze())\n",
    "instance_image = visualise_instances(predicted_instances, predicted_class, num_classes=5)\n",
    "axes[2, 1].set_title('Predicted instances')\n",
    "axes[2, 1].imshow(instance_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicity clear memory on GPU\n",
    "<small>Since all variables in the script are in the same scope, there is no garbage collection until reassignment. Need to get rid of derived data before running model subsequent times</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (logits, instance_embeddings, instance_image, image, labels,\n",
    "     instances, current_logits, current_labels, current_instances,\n",
    "     mask, label_embedding, predicted_class, predicted_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_original = torch.Tensor((plt.imread('test/010666874_816412_1428113.JPG') / 255).transpose(2, 0, 1)).unsqueeze(0)\n",
    "image = F.pad(image_original, (-16, -16, -37, -37))\n",
    "_, logits, instance_embeddings = model.forward_clean(image.cuda())\n",
    "current_logits = logits[0]\n",
    "predicted_class = F.pad(current_logits, (16, 16, 37, 37)).data.max(0)[1]\n",
    "instance_embeddings = F.pad(instance_embeddings, (16, 16, 37, 37))[0]\n",
    "\n",
    "predicted_instances = [None] * 5\n",
    "for class_index in range(5):\n",
    "    mask = predicted_class.view(-1) == class_index\n",
    "    if mask.max() > 0:\n",
    "        label_embedding = instance_embeddings.view(1, instance_embeddings.shape[0], -1)[..., mask]\n",
    "        label_embedding = label_embedding.data.cpu().numpy()[0]\n",
    "\n",
    "        predicted_instances[class_index] = mean_shift(label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(10, 12))\n",
    "for ax in axes: ax.axis('off')\n",
    "\n",
    "axes[0].set_title('Original image')\n",
    "axes[0].imshow(image_original[0].data.numpy().transpose(1, 2, 0))\n",
    "\n",
    "axes[1].set_title('Predicted classes')\n",
    "axes[1].imshow(predicted_class.cpu().numpy())\n",
    "\n",
    "axes[2].set_title('Predicted instances')\n",
    "axes[2].imshow(visualise_instances(predicted_instances, predicted_class, num_classes=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
